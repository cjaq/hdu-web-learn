{
    "basic_info": [
        {
            "type": "text",
            "detail": "标题:RMSProp"
        },
        {
            "type": "text",
            "detail": "作者:cja"
        },
        {
            "type": "text",
            "detail": "创建时间:2022-10-31"
        },
        {
            "type": "text",
            "detail": "所属模块:优化器模块"
        }
    ],
    "algorithm_describe": [
        {
            "type": "text",
            "detail": "RMSProp优化算法是AdaGrad算法的一种改进，因此在了解RMSPror之前必须要了解AdaGrad"
        },
        {
            "type": "text",
            "detail": "AdaGrad简单来讲，就是设置全局学习率之后，每次通过，全局学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同。起到的效果是在参数空间更为平缓的方向，因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小，并且能够使得陡峭的方向变得平缓，从而加快训练速度。"
        },
        {
            "type": "text",
            "detail": "RMSProp优化算法和AdaGrad算法唯一的不同，就在于累积平方梯度的求法不同。RMSProp算法不是像AdaGrad算法那样暴力直接的累加平方梯度，而是加了一个衰减系数来控制历史信息的获取多少。实验发现RMSProp在非凸条件下结果更好。"
        }
    ],
    "mathematical_principles": [
        {
            "type": "text",
            "detail": "下面是AdaGrad算法的过程"
        },
        {
            "type": "img",
            "detail": "res/images/adagrad.webp"
        },
        {
            "type": "text",
            "detail": "下面是RMSProp算法的过程"
        },
        {
            "type": "img",
            "detail": "res/images/rmsprop.webp"
        }
    ],
    "code_implement": [
        {
            "type": "text",
            "detail": "在pytorch中使用RMSProp非常简单,可以直接调用相关的实现"
        },
        {
            "type": "code",
            "detail": "import torch\n\nopt = torch.optim.RMSprop(model3.parameters())"
        }
    ],
    "experimental_performance":[
        {
            "type":"text",
            "detail":"实验记录了在CIFAR10上同样的卷积神经网络使用SGD, RMSProp, Adam的准确度的差距"
        },
        {
            "type": "table",
            "col": 5,
            "row": 4,
            "info": ["优化器类型", "iter:60000", "iter:120000", "iter:300000", "iter:600000",
                    "SGD",	"10.42%",	"14.58%",	"21.42%",	"27.10%",
                    "RMSP",	"24.13%",	"37.66%",	"44.58%",	"45.76%",
                    "Adam", "36.09%",   "45.80%",   "53.16%",   "57.89%"]
        }
    ],
    "others":[
        {
            "type":"text",
            "detail":"附录:实验代码如下"
        },
        {
            "type":"code",
            "detail":"import copy\n\nfrom torch import nn\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import transforms\nimport torch.nn.functional as F\nimport copy\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 16, 3, 1, 1),\n            nn.MaxPool2d(2, 2),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, 3, 1, 1),\n            nn.MaxPool2d(2, 2),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(32 * 8 * 8, 10)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\ndataloader = DataLoader(CIFAR10(r'E:\\data\\cifar10', train=True, transform=transforms.ToTensor()), batch_size=64, shuffle=False)\nmodel1 = CNN().cuda()\nmodel2 = copy.deepcopy(model1)\nmodel3 = copy.deepcopy(model1)\nopt1 = torch.optim.Adam(model1.parameters())\nopt2 = torch.optim.SGD(model2.parameters(), lr=1e-3)\nopt3 = torch.optim.RMSprop(model3.parameters())\n\nfor e in range(10):\n    acc_cnn_adam, acc_cnn_sgd, acc_cnn_rmsp = 0, 0, 0\n    for i, (x, y) in enumerate(dataloader):\n        x, y = x.to('cuda'), y.to('cuda')\n        out_cnn_adam = model1(x)\n        out_cnn_sgd = model2(x)\n        out_cnn_rmsp = model3(x)\n        acc_cnn_adam += torch.count_nonzero(torch.argmax(out_cnn_adam, dim=1) == y)\n        acc_cnn_sgd += torch.count_nonzero(torch.argmax(out_cnn_sgd, dim=1) == y)\n        acc_cnn_rmsp += torch.count_nonzero(torch.argmax(out_cnn_rmsp, dim=1) == y)\n        loss_cnn_adam = F.cross_entropy(out_cnn_adam, y)\n        loss_cnn_sgd = F.cross_entropy(out_cnn_sgd, y)\n        loss_cnn_rmsp = F.cross_entropy(out_cnn_rmsp, y)\n\n        opt1.zero_grad()\n        loss_cnn_adam.backward()\n        opt1.step()\n\n        opt2.zero_grad()\n        loss_cnn_sgd.backward()\n        opt2.step()\n\n        opt3.zero_grad()\n        loss_cnn_rmsp.backward()\n        opt3.step()\n    print(f'e:{e} acc_cnn_adam:{acc_cnn_adam/60000*100:.4f} acc_cnn_sgd:{acc_cnn_sgd/60000*100:.4f}'\n          f' acc_cnn_rmsp:{acc_cnn_rmsp/60000*100:.4f}')\n"
        }
    ]
}