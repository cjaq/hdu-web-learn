{
    "basic_info":[
        {
            "type": "text",
            "detail": "标题:残差神经网络"
        },
        {
            "type": "text",
            "detail": "作者:cja"
        },
        {
            "type": "text",
            "detail": "创建时间:2022-10-11"
        },
        {
            "type": "text",
            "detail": "所属模块:模型结构"
        }
    ],
    "algorithm_describe":[
        {
            "type": "text",
            "detail": "深度神经网络由于其具有很多层数（卷积层+全连接层）而冠以“深度”之名，基于神经网络的机器学习算法因此也被称作“深度学习”（由此可见AI从业者的包装能力之强）。然而随着算力的不断增强、数据集的不断扩张，一系列问题随之而来，例如："
        },
        {
            "type": "text",
            "detail": "1· 层数越多，训练效果一定越好吗？"
        },
        {
            "type": "text",
            "detail": "2· 如何优化过深的神经网络？"
        },
        {
            "type": "text",
            "detail": "3· 如何避免梯度消失和梯度爆炸？"
        },
        {
            "type": "text",
            "detail": "对于第一个问题，不假思索来看，在相同的优化条件下，更深神经网络的训练效果更强是十分显然的，然而由于参数过多、模型复杂度更高，深层神经网络会出现十分严重的过拟合问题，即训练集与测试集准确度之间的gap过大，由此引发了一系列的正则化方法，在此先按下不表。然而，何恺明在实际实验时发现，不仅是测试精度，而且在训练精度上，更深的（56层）神经网络也远不如浅一些的（20层）神经网络（如下图）。"
        },
        {
            "type": "img",
            "detail": "res/images/v2-8e9a544564acdb469185f355793469d7_720w.webp"
        },
        {
            "type": "text",
            "detail": "对于以上问题而言，除了十分棘手的梯度消失/梯度爆炸，以及过拟合的问题之外（以上议题将会在未来的更新中涉及），一个十分严峻的挑战在于负优化问题（degradation）。也就是说，56层的神经网络相比于20层，新增加的36层是对神经网络的“恶化”，它们非但没有起到自己应有的作用，反而扭曲了网络空间，升高了training error。由此一个想法自然而然的产生：如果这36层神经网络是恒等映射（identity mapping），那么56层的神经网络不就和20层的一样好了吗？更进一步呢？如果这36层神经网络相比于恒等映射再好上那么一点点（更接近最优函数），那么不就起到了正优化的作用了吗？ResNet的insight由此诞生。"
        }
    ],
    "mathematical_principles":[
        {
            "type": "text",
            "detail": "基于以上的考虑，我们在所拟合的函数中加入恒等函数。令F(x)为我们要拟合的函数,H(x)为某一层的最佳拟合函数,那么令F(x)和H(x)存在如下关系:"
        },
        {
            "type": "formula",
            "detail": "F(x) = H(x) - x"
        },
        {
            "type": "text",
            "detail": "残差块中的恒等函数部分，也被称为shortcut connection（or skip link)。对应到神经网络中，残差块的数学表达式可以写成："
        },
        {
            "type": "formula",
            "detail": "y = a(F(x,W)+x)"
        }
    ],
    "code_implement":[
        {
            "type": "text",
            "detail": "这里我们使用pytorch为基本框架来实现残差神经网络"
        },
        {
            "type": "code",
            "detail": "class res(nn.Module):\n    def __init__(self, inc, ouc):\n        super(res, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(inc, ouc, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(ouc, ouc, kernel_size=3, stride=1, padding=1),\n            nn.ReLU()\n        )\n\n        self.short_cut = nn.Conv2d(inc, ouc, 1)\n    def forward(self, x):\n        return self.model(x) + self.short_cut(x)\n\nclass RES(nn.Module):\n    def __init__(self):\n        super(RES, self).__init__()\n        self.model = nn.Sequential(\n            res(3, 16),\n            nn.MaxPool2d(2, 2),\n            res(16, 32),\n            nn.MaxPool2d(2, 2),\n            res(32, 32),\n            nn.Flatten(),\n            nn.Linear(32 * 8 * 8, 10)\n        )\n\n    def forward(self, x):\n        return self.model(x)"
        }
    ],
    "experimental_performance":[
        {
            "type":"text",
            "detail":"实验记录了在CIFAR10上残差网络相比较于卷积网络的优势"
        },
        {
            "type": "table",
            "col": 5,
            "row": 3,
            "info": ["模型结构", "iter:60000", "iter:120000", "iter:300000", "iter:600000",
                    "卷积神经网络",	"30.72%",	"43.15%",	"54.91%",	"61.09%",
                    "残差神经网络",	"36.14%",	"46.69%",	"58.57%",	"65.80%"]
        }
    ],
    "others":[
        {
            "type":"text",
            "detail":"附录:实验代码如下"
        },
        {
            "type":"code",
            "detail":"import copy\n\nfrom torch import nn\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import transforms\nimport torch.nn.functional as F\nimport copy\n\n\nclass res(nn.Module):\n    def __init__(self, inc, ouc):\n        super(res, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(inc, ouc, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(ouc, ouc, kernel_size=3, stride=1, padding=1),\n            nn.ReLU()\n        )\n\n        self.short_cut = nn.Conv2d(inc, ouc, 1)\n    def forward(self, x):\n        return self.model(x) + self.short_cut(x)\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 16, 3, 1, 1),\n            nn.MaxPool2d(2, 2),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, 3, 1, 1),\n            nn.MaxPool2d(2, 2),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, 3, 1, 1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(32 * 8 * 8, 10)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass RES(nn.Module):\n    def __init__(self):\n        super(RES, self).__init__()\n        self.model = nn.Sequential(\n            res(3, 16),\n            nn.MaxPool2d(2, 2),\n            res(16, 32),\n            nn.MaxPool2d(2, 2),\n            res(32, 32),\n            nn.Flatten(),\n            nn.Linear(32 * 8 * 8, 10)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\n\ndataloader = DataLoader(CIFAR10(r'E:\\data\\cifar10', train=True, transform=transforms.ToTensor()), batch_size=64, shuffle=False)\nmodel1 = CNN().cuda()\nmodel2 = RES().cuda()\nopt1 = torch.optim.Adam(model1.parameters())\nopt2 = torch.optim.Adam(model2.parameters())\n\nfor e in range(10):\n    acc_cnn, acc_res = 0, 0\n    for i, (x, y) in enumerate(dataloader):\n        x, y = x.to('cuda'), y.to('cuda')\n        out_cnn = model1(x)\n        out_res = model2(x)\n        acc_cnn += torch.count_nonzero(torch.argmax(out_cnn, dim=1) == y)\n        acc_res += torch.count_nonzero(torch.argmax(out_res, dim=1) == y)\n        loss_cnn_adam = F.cross_entropy(out_cnn, y)\n        loss_cnn_sgd = F.cross_entropy(out_res, y)\n\n        opt1.zero_grad()\n        loss_cnn_adam.backward()\n        opt1.step()\n\n        opt2.zero_grad()\n        loss_cnn_sgd.backward()\n        opt2.step()\n\n    print(f'e:{e} acc_cnn:{acc_cnn/60000*100:.4f} acc_res:{acc_res/60000*100:.4f}')\n"
        }
    ]
}